{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Consulting project: Hack Data\n",
        "### A company in San Francisco have been recently hacked and need your help finding out about the hackers! The technology firm has 3 potential hackers that perpetrated the attack. They are certain of the first two hackers but they aren't very sure if the third hacker was involved or not.\n",
        "\n",
        "### Can you help figure out whether or not the third suspect had anything to do with the attacks, or was it just two hackers? Hint: Each hacker should have roughly the same amount of attacks."
      ],
      "metadata": {
        "id": "MXyjWij2FZXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up Spark and load data"
      ],
      "metadata": {
        "id": "106DmScAFeoC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyWjqitA2ie8",
        "outputId": "ecdf2488-7b76-4539-f0d9-a0ce96814ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [30.0 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,348 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,519 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,307 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [22.9 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [40.8 kB]\n",
            "Fetched 6,552 kB in 8s (783 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "appname = \"K-Means Clustering - Seeds\"\n",
        "\n",
        "# Look into https://spark.apache.org/downloads.html for the latest version\n",
        "spark_mirror = \"https://mirrors.sonic.net/apache/spark\"\n",
        "spark_version = \"3.3.1\"\n",
        "hadoop_version = \"3\"\n",
        "\n",
        "# Install Java 8 (Spark does not work with newer Java versions)\n",
        "! apt-get update\n",
        "! apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and extract Spark binary distribution\n",
        "! rm -rf spark-{spark_version}-bin-hadoop{hadoop_version}.tgz spark-{spark_version}-bin-hadoop{hadoop_version}\n",
        "! wget -q {spark_mirror}/spark-{spark_version}/spark-{spark_version}-bin-hadoop{hadoop_version}.tgz\n",
        "! tar xzf spark-{spark_version}-bin-hadoop{hadoop_version}.tgz\n",
        "\n",
        "# The only 2 environment variables needed to set up Java and Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-{spark_version}-bin-hadoop{hadoop_version}\"\n",
        "\n",
        "# Set up the Spark environment based on the environment variable SPARK_HOME \n",
        "! pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Get the Spark session object (basic entry point for every operation)\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(appname).master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIRZZLVRgTCc",
        "outputId": "4e9a1a0d-1aed-46f8-d22b-febad2c16c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D_8en_XCgAxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c8c8e9-05d5-46ca-dc8c-738ff17073dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KAGGLE_CONFIG_DIR\"] = \"/content/drive/MyDrive/kaggle\"\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "! rm -f hack_data.csv\n",
        "api.dataset_download_file(\"soheiltehranipour/sample-hack-data\", \"Hack_Data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary analysis and data preprocessing"
      ],
      "metadata": {
        "id": "eIHyNtZ9Fsnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVx1klzHVNKc",
        "outputId": "b740bf46-fd20-4490-a529-2da1968daa66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
            "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|\n",
            "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
            "|                      8|           391.09|              1|             2.96|              7|            Slovenia|           72.37|\n",
            "|                     20|           720.99|              0|             3.04|              9|British Virgin Is...|           69.08|\n",
            "|                     31|           356.32|              1|             3.71|              8|             Tokelau|           70.58|\n",
            "|                      2|           228.08|              1|             2.48|              8|             Bolivia|            70.8|\n",
            "|                     20|            408.5|              0|             3.57|              8|                Iraq|           71.28|\n",
            "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "df = spark.read.format('csv').options(inferSchema=True, header=True).load('hack_data.csv')\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ybpJQux21EJ",
        "outputId": "75911836-c42a-4506-98b2-de86a51499cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Session_Connection_Time: integer (nullable = true)\n",
            " |-- Bytes Transferred: double (nullable = true)\n",
            " |-- Kali_Trace_Used: integer (nullable = true)\n",
            " |-- Servers_Corrupted: double (nullable = true)\n",
            " |-- Pages_Corrupted: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            " |-- WPM_Typing_Speed: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzFEoAedLaCy",
        "outputId": "522142e3-9f7d-4880-feda-422dd20f7227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|summary|Session_Connection_Time| Bytes Transferred|   Kali_Trace_Used|Servers_Corrupted|   Pages_Corrupted|   Location|  WPM_Typing_Speed|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "|  count|                    334|               334|               334|              334|               334|        334|               334|\n",
            "|   mean|     30.008982035928145| 607.2452694610777|0.5119760479041916|5.258502994011977|10.838323353293413|       null|57.342395209580864|\n",
            "| stddev|     14.088200614636158|286.33593163576757|0.5006065264451406| 2.30190693339697|  3.06352633036022|       null| 13.41106336843464|\n",
            "|    min|                      1|              10.0|                 0|              1.0|                 6|Afghanistan|              40.0|\n",
            "|    max|                     60|            1330.5|                 1|             10.0|                15|   Zimbabwe|              75.0|\n",
            "+-------+-----------------------+------------------+------------------+-----------------+------------------+-----------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Statistical description of features\n",
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count null values within the data columns\n",
        "print({col: df.filter(df[col].isNull()).count() for col in df.columns})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m64gokoUGGYr",
        "outputId": "adfff3d6-1109-47ce-ea50-666f4b653564"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Session_Connection_Time': 0, 'Bytes Transferred': 0, 'Kali_Trace_Used': 0, 'Servers_Corrupted': 0, 'Pages_Corrupted': 0, 'Location': 0, 'WPM_Typing_Speed': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we observe that there is a string feature within the data, the Location. However, this variable might be useless since hackers used VPNs, therefore, it will be discarded. Before we build our model, we first need to transform categorical features using One Hot Encoder."
      ],
      "metadata": {
        "id": "MPUa1TZYcKkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(dropLast = False, handleInvalid = \"error\", inputCol = \"Kali_Trace_Used\",\n",
        "                        outputCol = \"Kali_Trace_Used_OHE\")\n"
      ],
      "metadata": {
        "id": "yXk8fvNxIMf4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we assemble the feature set into a single vector."
      ],
      "metadata": {
        "id": "L2_BRZHpKjhc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbk9coV2HSsN"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used_OHE',\n",
        "                                       'Servers_Corrupted', 'Pages_Corrupted', 'WPM_Typing_Speed']\n",
        "                            , outputCol='features', handleInvalid = \"skip\")\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, before building our model we need to scale our data, since each feature has a different scale. For this, I will use Standard Scaler, and the different transformers will be assembled in a single Pipeline to simplify the preprocessing of data.\n"
      ],
      "metadata": {
        "id": "_ap0-BZGMgRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAXyAsewLqum",
        "outputId": "50623515-04ed-49b2-ae94-c688052705c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|            features|     scaled_features|\n",
            "+--------------------+--------------------+\n",
            "|[8.0,391.09,0.0,1...|[0.56785108466505...|\n",
            "|[20.0,720.99,1.0,...|[1.41962771166263...|\n",
            "|[31.0,356.32,0.0,...|[2.20042295307707...|\n",
            "|[2.0,228.08,0.0,1...|[0.14196277116626...|\n",
            "|[20.0,408.5,1.0,0...|[1.41962771166263...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.pipeline import Pipeline\n",
        "\n",
        "scaler = StandardScaler(inputCol='features', outputCol='scaled_features')\n",
        "\n",
        "pipeline = Pipeline(stages = [encoder, assembler, scaler]).fit(df)\n",
        "\n",
        "scaled_df = pipeline.transform(df)\n",
        "scaled_df.select([\"features\", \"scaled_features\"]).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfv0E8cLMNAd"
      },
      "source": [
        "## Building the K-Means clustering model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first build the K-means model."
      ],
      "metadata": {
        "id": "iPKiikax4j4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "yr6krMiYXBcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c38759-bcb9-4127-c1ef-c2d9fbabdd2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean, current: euclidean)\n",
            "featuresCol: features column name. (default: features, current: scaled_features)\n",
            "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||, current: k-means||)\n",
            "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2, current: 50)\n",
            "k: The number of clusters to create. Must be > 1. (default: 2)\n",
            "maxIter: max number of iterations (>= 0). (default: 20, current: 100)\n",
            "predictionCol: prediction column name. (default: prediction)\n",
            "seed: random seed. (default: -5706602770492230126)\n",
            "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
            "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "kmeans = KMeans(featuresCol='scaled_features', distanceMeasure = \"euclidean\",\n",
        "                initMode = \"k-means||\", initSteps = 50, maxIter = 100)\n",
        "\n",
        "print(kmeans.explainParams())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to estimate the optimal number of clusters I will perform a Cross Validation using different number of clusters. Every model will be evaluated according to their Silhouette score and validated using 5-fold cross validation method."
      ],
      "metadata": {
        "id": "vsTE9uRd2QFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Create an evaluator for our model\n",
        "evaluator = ClusteringEvaluator(metricName='silhouette', distanceMeasure='squaredEuclidean',\n",
        "                                featuresCol = \"scaled_features\", predictionCol = \"prediction\")\n",
        "\n",
        "# Create a grip of parameters for different number of clusters (K)\n",
        "params = ParamGridBuilder().addGrid(kmeans.k, range(2,11)).build()\n",
        "\n",
        "# Perform Cross-Validation to find the best model\n",
        "crossval = CrossValidator(estimator=kmeans,\n",
        "                          estimatorParamMaps=params,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=5)\n",
        "\n",
        "# Train the models with our scaled data\n",
        "best_km = crossval.fit(scaled_df)"
      ],
      "metadata": {
        "id": "MZOu_PyluX1a"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We analyse the scores of the different models compared at the Cross Validation process and evaluate the final Silhouette score of our optimised model."
      ],
      "metadata": {
        "id": "_QhvyHSZ7eka"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IxdcQHM3XSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0723289-d904-45aa-d2b4-6a6170dcfc71"
      },
      "source": [
        "for i in range(len(best_km.avgMetrics)):\n",
        "    print(list(best_km.getEstimatorParamMaps()[i].keys())[0], \"=\", list(best_km.getEstimatorParamMaps()[i].values())[0])\n",
        "    print(\"Silhouette score\", round(best_km.avgMetrics[i], 3))\n",
        "    print()\n",
        "\n",
        "# Return best K value\n",
        "import numpy as np\n",
        "for param, value in best_km.getEstimatorParamMaps()[np.argmax(best_km.avgMetrics)].items():\n",
        "  print(\"|--------------------------|\\n\" +\n",
        "        \"  Best parameter\\n\"f\"  {param}: {value}\",\n",
        "        \"\\n\\n  Silhouette score\\n\"f\"  {max(best_km.avgMetrics)}\")"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans_914085005846__k = 2\n",
            "Silhouette score 0.624\n",
            "\n",
            "KMeans_914085005846__k = 3\n",
            "Silhouette score 0.759\n",
            "\n",
            "KMeans_914085005846__k = 4\n",
            "Silhouette score 0.677\n",
            "\n",
            "KMeans_914085005846__k = 5\n",
            "Silhouette score 0.709\n",
            "\n",
            "KMeans_914085005846__k = 6\n",
            "Silhouette score 0.586\n",
            "\n",
            "KMeans_914085005846__k = 7\n",
            "Silhouette score 0.547\n",
            "\n",
            "KMeans_914085005846__k = 8\n",
            "Silhouette score 0.434\n",
            "\n",
            "KMeans_914085005846__k = 9\n",
            "Silhouette score 0.394\n",
            "\n",
            "KMeans_914085005846__k = 10\n",
            "Silhouette score 0.341\n",
            "\n",
            "|--------------------------|\n",
            "  Best parameter\n",
            "  KMeans_914085005846__k: 3 \n",
            "\n",
            "  Silhouette score\n",
            "  0.7586589413351843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the best model (which uses the optimal combinantion of parameters) to predict the labels for our data."
      ],
      "metadata": {
        "id": "GhmmdBLu4zpK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j2Pf4-N3JuQ",
        "outputId": "c1775a7a-200f-4eb9-cb5a-e86d4aac17f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model => KMeansModel: uid=KMeans_914085005846, k=3, distanceMeasure=euclidean, numFeatures=7\n",
            "\n",
            "+--------------------+--------------------+----------+\n",
            "|            features|     scaled_features|prediction|\n",
            "+--------------------+--------------------+----------+\n",
            "|[8.0,391.09,0.0,1...|[0.56785108466505...|         1|\n",
            "|[20.0,720.99,1.0,...|[1.41962771166263...|         1|\n",
            "|[31.0,356.32,0.0,...|[2.20042295307707...|         1|\n",
            "|[2.0,228.08,0.0,1...|[0.14196277116626...|         1|\n",
            "|[20.0,408.5,1.0,0...|[1.41962771166263...|         1|\n",
            "|[1.0,390.69,0.0,1...|[0.07098138558313...|         1|\n",
            "|[18.0,342.97,0.0,...|[1.27766494049636...|         1|\n",
            "|[22.0,101.61,0.0,...|[1.56159048282889...|         1|\n",
            "|[15.0,275.53,0.0,...|[1.06472078374697...|         1|\n",
            "|[12.0,424.83,0.0,...|[0.85177662699757...|         1|\n",
            "+--------------------+--------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ],
      "source": [
        "print(\"Best model =>\",best_km.bestModel)\n",
        "print()\n",
        "\n",
        "# Get predictions using the best model (K = 3)\n",
        "pred = best_km.transform(scaled_df)\n",
        "pred.select([\"features\", \"scaled_features\", \"prediction\"]).show(10)\n",
        "\n",
        "best_km.bestModel."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I will compare the total number of observations within each cluster."
      ],
      "metadata": {
        "id": "yE9c7PVkFt5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Observations in K = 3 clusters\")\n",
        "{\"cluster \"+str(cluster): pred.filter(pred.prediction == cluster).count() for cluster in range(best_km.bestModel.getK())}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6KpHI98BlGr",
        "outputId": "664f7756-aa78-4c30-acfd-2753feaa75ca"
      },
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observations in K = 3 clusters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cluster 0': 88, 'cluster 1': 167, 'cluster 2': 79}"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans2 = KMeans(featuresCol='scaled_features', distanceMeasure = \"euclidean\",\n",
        "                initMode = \"k-means||\", initSteps = 50, maxIter = 100,\n",
        "                k = 2).fit(scaled_df)\n",
        "\n",
        "pred2 = kmeans2.transform(scaled_df)\n",
        "\n",
        "print(\"Observations in K = 2 clusters\")\n",
        "{\"cluster \"+str(cluster): pred2.filter(pred2.prediction == cluster).count() for cluster in range(kmeans2.getK())}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWTesEjgH3EV",
        "outputId": "399f208a-e82e-4a15-d9e4-7c4ad763ac6a"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observations in K = 2 clusters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cluster 0': 167, 'cluster 1': 167}"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, wven though the model with number of clusters K = 3 is the one with the highest Silhouette score, there is an unequal distribution of observations among the 3 final clusters. In contrast, if we cluster our data using K = 2, the cluster size is equal for both clusters.\n",
        "\n",
        "In view of this, and considering that each hacker should have roughly the same amount of attacks, we can conclude that probably two different hackers performed the attack."
      ],
      "metadata": {
        "id": "l8JGqioYJc9I"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "106DmScAFeoC",
        "eIHyNtZ9Fsnt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}